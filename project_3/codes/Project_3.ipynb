{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this project, reviews from two types of products are classified. Reviews of movies and books downloaded from Reddit.com. The two groups of reviews were combined in one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from bs4 import BeautifulSoup\n",
    "import regex as re\n",
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB,ComplementNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets were downloaded in \"get_data.ipynb\" and were saved in \"my_books.csv\" and \"my_movies.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_books = pd.read_csv('my_books.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movies = pd.read_csv('my_movies.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>author</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>is_self</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>I cant seem to remeber this movie</td>\n",
       "      <td>theres this movie I watched as a child but I c...</td>\n",
       "      <td>movies</td>\n",
       "      <td>1568141984</td>\n",
       "      <td>Trystan_03</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>9/10/19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Dunkin' Donuts in 80s movies</td>\n",
       "      <td>Random question I know, but does anybody every...</td>\n",
       "      <td>movies</td>\n",
       "      <td>1568142543</td>\n",
       "      <td>DoctorWernerKlopek</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>9/10/19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>IT Chapter 2 - Ending Question [Spoilers]</td>\n",
       "      <td>I recently saw IT Chp 2 and I was confused by ...</td>\n",
       "      <td>movies</td>\n",
       "      <td>1568143828</td>\n",
       "      <td>GregHauser</td>\n",
       "      <td>33</td>\n",
       "      <td>20</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>9/10/19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>It's PG now and he wouldn't have to lose all t...</td>\n",
       "      <td>Imagine if Joaquin Phoenix were in a movie cal...</td>\n",
       "      <td>movies</td>\n",
       "      <td>1568143837</td>\n",
       "      <td>BlkSunshineRdriguez</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>9/10/19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Is \"The Man Who Killed Hitler and Then the Big...</td>\n",
       "      <td>Hi Everyone, I just watched \"The Man Who Kille...</td>\n",
       "      <td>movies</td>\n",
       "      <td>1568144143</td>\n",
       "      <td>TheFirstHunter</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>9/10/19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>He wouldn't have to lose all that weight</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>movies</td>\n",
       "      <td>1568144475</td>\n",
       "      <td>BlkSunshineRdriguez</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>9/10/19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Todd Phillips Shuts Down Rumor ‘Joker’ Sequel ...</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>movies</td>\n",
       "      <td>1568144488</td>\n",
       "      <td>Goanimest</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>9/10/19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>How scary is pet semetary 2019?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>movies</td>\n",
       "      <td>1568144581</td>\n",
       "      <td>Legit_king_yolo</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>9/10/19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>What is your favorite on screen kiss?</td>\n",
       "      <td>On screen chemistry can make or break a movie....</td>\n",
       "      <td>movies</td>\n",
       "      <td>1568145491</td>\n",
       "      <td>bothanspied</td>\n",
       "      <td>73</td>\n",
       "      <td>29</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>9/10/19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>Sites to follow for indie movie news?</td>\n",
       "      <td>What sites/twitters/pages/whatever do y’all fo...</td>\n",
       "      <td>movies</td>\n",
       "      <td>1568145845</td>\n",
       "      <td>NoTakaru</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>9/10/19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>Some Good Movies I Watched This Past Week (and...</td>\n",
       "      <td>Girl, Interrupted- an instant favorite. Excell...</td>\n",
       "      <td>movies</td>\n",
       "      <td>1568146302</td>\n",
       "      <td>Fred_Baratheon</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>9/10/19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>What is your favorite film from 2013?</td>\n",
       "      <td>I'll keep it brief to let the discussions ensu...</td>\n",
       "      <td>movies</td>\n",
       "      <td>1568146535</td>\n",
       "      <td>HaydenJLewis</td>\n",
       "      <td>156</td>\n",
       "      <td>8</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>9/10/19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>My thoughts on It: Chapter Two</td>\n",
       "      <td>I really enjoyed the short DC Promo they showe...</td>\n",
       "      <td>movies</td>\n",
       "      <td>1568148223</td>\n",
       "      <td>myztero</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>9/10/19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>Theater Employees, What Movies Played To Almos...</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>movies</td>\n",
       "      <td>1568148592</td>\n",
       "      <td>wutsyovision</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>9/10/19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>The Joker movie just doesn’t look that interes...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>movies</td>\n",
       "      <td>1568148615</td>\n",
       "      <td>Coozhound</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>9/10/19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>I just saw Once upon a time in Hollywood</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>movies</td>\n",
       "      <td>1568148750</td>\n",
       "      <td>Curlyinger</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>9/10/19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>Goldfinch (2019) cancelled?</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>movies</td>\n",
       "      <td>1568148785</td>\n",
       "      <td>Sunlightknight7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>9/10/19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>I need to talk with you guys about my intense ...</td>\n",
       "      <td>I am 18 years old and I have loved movies almo...</td>\n",
       "      <td>movies</td>\n",
       "      <td>1568149094</td>\n",
       "      <td>Superman38458</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>9/10/19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>Which Scenes Have Traumatized You As A Child?</td>\n",
       "      <td>For me, the greatest example would have to be ...</td>\n",
       "      <td>movies</td>\n",
       "      <td>1568149446</td>\n",
       "      <td>Gattsu2000</td>\n",
       "      <td>75</td>\n",
       "      <td>12</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>9/10/19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>[Discussion] Jenny baby-trapped Forrest Gump</td>\n",
       "      <td>Watched Forrest Gump last night for the 30th t...</td>\n",
       "      <td>movies</td>\n",
       "      <td>1568149610</td>\n",
       "      <td>lawdoggingit</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>9/10/19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  \\\n",
       "0                   I cant seem to remeber this movie   \n",
       "1                        Dunkin' Donuts in 80s movies   \n",
       "2           IT Chapter 2 - Ending Question [Spoilers]   \n",
       "3   It's PG now and he wouldn't have to lose all t...   \n",
       "4   Is \"The Man Who Killed Hitler and Then the Big...   \n",
       "5            He wouldn't have to lose all that weight   \n",
       "6   Todd Phillips Shuts Down Rumor ‘Joker’ Sequel ...   \n",
       "7                     How scary is pet semetary 2019?   \n",
       "8               What is your favorite on screen kiss?   \n",
       "9               Sites to follow for indie movie news?   \n",
       "10  Some Good Movies I Watched This Past Week (and...   \n",
       "11              What is your favorite film from 2013?   \n",
       "12                     My thoughts on It: Chapter Two   \n",
       "13  Theater Employees, What Movies Played To Almos...   \n",
       "14  The Joker movie just doesn’t look that interes...   \n",
       "15           I just saw Once upon a time in Hollywood   \n",
       "16                        Goldfinch (2019) cancelled?   \n",
       "17  I need to talk with you guys about my intense ...   \n",
       "18      Which Scenes Have Traumatized You As A Child?   \n",
       "19       [Discussion] Jenny baby-trapped Forrest Gump   \n",
       "\n",
       "                                             selftext subreddit created_utc  \\\n",
       "0   theres this movie I watched as a child but I c...    movies  1568141984   \n",
       "1   Random question I know, but does anybody every...    movies  1568142543   \n",
       "2   I recently saw IT Chp 2 and I was confused by ...    movies  1568143828   \n",
       "3   Imagine if Joaquin Phoenix were in a movie cal...    movies  1568143837   \n",
       "4   Hi Everyone, I just watched \"The Man Who Kille...    movies  1568144143   \n",
       "5                                           [removed]    movies  1568144475   \n",
       "6                                           [removed]    movies  1568144488   \n",
       "7                                                 NaN    movies  1568144581   \n",
       "8   On screen chemistry can make or break a movie....    movies  1568145491   \n",
       "9   What sites/twitters/pages/whatever do y’all fo...    movies  1568145845   \n",
       "10  Girl, Interrupted- an instant favorite. Excell...    movies  1568146302   \n",
       "11  I'll keep it brief to let the discussions ensu...    movies  1568146535   \n",
       "12  I really enjoyed the short DC Promo they showe...    movies  1568148223   \n",
       "13                                          [removed]    movies  1568148592   \n",
       "14                                                NaN    movies  1568148615   \n",
       "15                                          [removed]    movies  1568148750   \n",
       "16                                          [removed]    movies  1568148785   \n",
       "17  I am 18 years old and I have loved movies almo...    movies  1568149094   \n",
       "18  For me, the greatest example would have to be ...    movies  1568149446   \n",
       "19  Watched Forrest Gump last night for the 30th t...    movies  1568149610   \n",
       "\n",
       "                 author num_comments score is_self timestamp  \n",
       "0            Trystan_03            0     1    TRUE   9/10/19  \n",
       "1    DoctorWernerKlopek           10     2    TRUE   9/10/19  \n",
       "2            GregHauser           33    20    TRUE   9/10/19  \n",
       "3   BlkSunshineRdriguez            0     1    TRUE   9/10/19  \n",
       "4        TheFirstHunter           18     3    TRUE   9/10/19  \n",
       "5   BlkSunshineRdriguez            0     1    TRUE   9/10/19  \n",
       "6             Goanimest            1     1    TRUE   9/10/19  \n",
       "7       Legit_king_yolo            0     1    TRUE   9/10/19  \n",
       "8           bothanspied           73    29    TRUE   9/10/19  \n",
       "9              NoTakaru            1     4    TRUE   9/10/19  \n",
       "10       Fred_Baratheon            5     0    TRUE   9/10/19  \n",
       "11         HaydenJLewis          156     8    TRUE   9/10/19  \n",
       "12              myztero            4     0    TRUE   9/10/19  \n",
       "13         wutsyovision            0     1    TRUE   9/10/19  \n",
       "14            Coozhound            0     1    TRUE   9/10/19  \n",
       "15           Curlyinger            2     1    TRUE   9/10/19  \n",
       "16      Sunlightknight7            0     1    TRUE   9/10/19  \n",
       "17        Superman38458            1     1    TRUE   9/10/19  \n",
       "18           Gattsu2000           75    12    TRUE   9/10/19  \n",
       "19         lawdoggingit           18     0    TRUE   9/10/19  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_movies.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_books, df_movies])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32270, 9)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title             11\n",
       "selftext        3770\n",
       "subreddit         18\n",
       "created_utc       21\n",
       "author            22\n",
       "num_comments      22\n",
       "score             22\n",
       "is_self           23\n",
       "timestamp         26\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Check for missing data\n",
    "\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop rows with missing data\n",
    "\n",
    "df.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title           0\n",
       "selftext        0\n",
       "subreddit       0\n",
       "created_utc     0\n",
       "author          0\n",
       "num_comments    0\n",
       "score           0\n",
       "is_self         0\n",
       "timestamp       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove lines with removed and deleted contents\n",
    "\n",
    "df = df[(df['selftext'] != '[removed]') & ((df['selftext'] !=  '[deleted]') )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    9227\n",
       "0    7268\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['subreddit'].replace('books', '0', inplace = True)\n",
    "df['subreddit'].replace('movies', '1', inplace = True)\n",
    "df['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16261, 10)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sorting by first review \n",
    "df.sort_values(\"selftext\", inplace=True) \n",
    "  \n",
    "# dropping duplicate values \n",
    "df.drop_duplicates(subset =\"selftext\", \n",
    "                     keep = False, inplace = True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12195, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set X and y\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[['selftext']],\n",
    "                                                    df['subreddit'],\n",
    "                                                    test_size = 0.25,\n",
    "                                                    random_state = 42)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-24T15:41:00.960134Z",
     "start_time": "2017-10-24T15:41:00.951205Z"
    }
   },
   "outputs": [],
   "source": [
    "## Create a function to transform all reviews to stirng of words\n",
    "\n",
    "def review_to_words(raw_review):\n",
    "    # Function to convert a raw review to a string of words\n",
    "    # The input is a single string (a raw movie review), and \n",
    "    # the output is a single string (a preprocessed movie review)\n",
    "    \n",
    "    # 1. Remove HTML.\n",
    "    review_text = BeautifulSoup(raw_review).get_text()\n",
    "    \n",
    "    # 2. Remove non-letters.\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text)\n",
    "    \n",
    "    # 3. Convert to lower case, split into individual words.\n",
    "    words = letters_only.lower().split()\n",
    "    \n",
    "    # 4. In Python, searching a set is much faster than searching\n",
    "    # a list, so convert the stop words to a set.\n",
    "    stops = set(stopwords.words('english'))\n",
    "    \n",
    "    # 5. Remove stop words.\n",
    "    meaningful_words = [w for w in words if not w in stops]\n",
    "    \n",
    "    # 6. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return(\" \".join(meaningful_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-24T15:41:04.730461Z",
     "start_time": "2017-10-24T15:41:04.715975Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 16261 reviews.\n"
     ]
    }
   ],
   "source": [
    "# Get the number of reviews based on the dataframe size.\n",
    "total_reviews = df.shape[0]\n",
    "print(f'There are {total_reviews} reviews.')\n",
    "\n",
    "# Initialize an empty list to hold the clean reviews.\n",
    "clean_train = []\n",
    "clean_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-24T15:41:46.357830Z",
     "start_time": "2017-10-24T15:41:20.317753Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and parsing the training set movie & book reviews...\n",
      "Cleaning and parsing the testing set movie & book reviews...\n"
     ]
    }
   ],
   "source": [
    "print(\"Cleaning and parsing the training set movie & book reviews...\")\n",
    "\n",
    "j = 0\n",
    "for train in X_train['selftext']:\n",
    "    # Convert review to words, then append to clean_train.\n",
    "    clean_train.append(review_to_words(train))\n",
    "\n",
    "    \n",
    "    j += 1\n",
    "\n",
    "\n",
    "# Let's do the same for our testing set.\n",
    "\n",
    "print(\"Cleaning and parsing the testing set movie & book reviews...\")\n",
    "\n",
    "for test in X_test['selftext']:\n",
    "    # Convert review to words, then append to clean_train.\n",
    "    clean_test.append(review_to_words(test))\n",
    "\n",
    "    \n",
    "    j += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "import regex as re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-24T15:42:31.247194Z",
     "start_time": "2017-10-24T15:42:27.791843Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import CountVectorizer.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Instantiate the \"CountVectorizer\" object, which is scikit-learn's\n",
    "# bag of words tool.\n",
    "vectorizer = CountVectorizer(analyzer = \"word\",\n",
    "                             tokenizer = None,\n",
    "                             preprocessor = None,\n",
    "                             stop_words = None,\n",
    "                             max_features =100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12195, 100)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit_transform the data\n",
    "\n",
    "train_data_features = vectorizer.fit_transform(clean_train)\n",
    "\n",
    "test_data_features = vectorizer.transform(clean_test)\n",
    "\n",
    "# convert the data into array\n",
    "\n",
    "\n",
    "train_data_features = train_data_features.toarray()\n",
    "test_data_features = test_data_features.toarray()\n",
    "train_data_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start classification!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-25T09:59:36.116770Z",
     "start_time": "2017-10-25T09:59:36.104354Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import logistic regression.\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate logistic regression model.\n",
    "\n",
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12195, 100)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(train_data_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9425174251742517"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate model on training data.\n",
    "lr.score(train_data_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9313821938022626"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate model on testing data.\n",
    "\n",
    "lr.score(test_data_features, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.590507\n",
       "0    0.409493\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_LR = lr.predict(test_data_features)\n",
    "pred_LR = pd.DataFrame(pred_LR)\n",
    "pred_LR[0].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform GridSearch to determine te best Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0003640058615373581"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.coef_.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13755                                      I'm listening. \n",
       "9317     I was watching Beauty and The Beast the other ...\n",
       "13247    If so, can you tell me if it's worth seeing to...\n",
       "4345     I started reading War and Peace (Maude transla...\n",
       "3827     I got myself some page markers since I want to...\n",
       "                               ...                        \n",
       "30       The Far Pavillions was a bit too expensive for...\n",
       "1944     I have to read this for my AP Language and Com...\n",
       "9762     After the final scene, if the movie were to co...\n",
       "4600     for procedural single book, I guess it works o...\n",
       "15713    I think I saw it on IFC or the Sundance channe...\n",
       "Name: selftext, Length: 12195, dtype: object"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train['selftext']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set it up with two stages:\n",
    "# 1. An instance of CountVectorizer (transformer)\n",
    "# 2. A LogisticRegression instance (estimator)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('feats', FeatureUnion([\n",
    "         ('tfidf', TfidfVectorizer()),\n",
    "         ('cvec', CountVectorizer()),\n",
    "    ])),\n",
    "    ('nb', MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search over the following values of hyperparameters:\n",
    "\n",
    "pipe_params = {\n",
    "    'feats__cvec__max_features': [5000],\n",
    "    'feats__cvec__max_df': [0.6,0.8],\n",
    "    'feats__cvec__ngram_range': [(1,1)],\n",
    "    'feats__cvec__stop_words' :['english', None],\n",
    "    'feats__tfidf__max_features': [100],\n",
    "    'feats__tfidf__max_df': [0.8,.9],\n",
    "    'feats__tfidf__ngram_range': [(1,2)],\n",
    "    'feats__tfidf__stop_words' :['english', None],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate GridSearchCV.\n",
    "\n",
    "gs = GridSearchCV(pipe, # what object are we optimizing?\n",
    "                  param_grid=pipe_params, # what parameters values are we searching?\n",
    "                  cv=3,\n",
    "                  n_jobs=-1,\n",
    "                  verbose=2) # 3-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done  48 out of  48 | elapsed:  1.5min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('feats',\n",
       "                                        FeatureUnion(n_jobs=None,\n",
       "                                                     transformer_list=[('tfidf',\n",
       "                                                                        TfidfVectorizer(analyzer='word',\n",
       "                                                                                        binary=False,\n",
       "                                                                                        decode_error='strict',\n",
       "                                                                                        dtype=<class 'numpy.float64'>,\n",
       "                                                                                        encoding='utf-8',\n",
       "                                                                                        input='content',\n",
       "                                                                                        lowercase=True,\n",
       "                                                                                        max_df=1.0,\n",
       "                                                                                        max_features=None,\n",
       "                                                                                        min_df=1,\n",
       "                                                                                        ngram_range=(1,\n",
       "                                                                                                     1),\n",
       "                                                                                        n...\n",
       "             param_grid={'feats__cvec__max_df': [0.6, 0.8],\n",
       "                         'feats__cvec__max_features': [5000],\n",
       "                         'feats__cvec__ngram_range': [(1, 1)],\n",
       "                         'feats__cvec__stop_words': ['english', None],\n",
       "                         'feats__tfidf__max_df': [0.8, 0.9],\n",
       "                         'feats__tfidf__max_features': [100],\n",
       "                         'feats__tfidf__ngram_range': [(1, 2)],\n",
       "                         'feats__tfidf__stop_words': ['english', None]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=2)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit GridSearch to training data.\n",
    "gs.fit(X_train['selftext'], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feats__cvec__max_df': 0.6,\n",
       " 'feats__cvec__max_features': 5000,\n",
       " 'feats__cvec__ngram_range': (1, 1),\n",
       " 'feats__cvec__stop_words': 'english',\n",
       " 'feats__tfidf__max_df': 0.9,\n",
       " 'feats__tfidf__max_features': 100,\n",
       " 'feats__tfidf__ngram_range': (1, 2),\n",
       " 'feats__tfidf__stop_words': None}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9553915539155392\n"
     ]
    }
   ],
   "source": [
    "# What's the best score?\n",
    "print(gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('feats',\n",
       "                 FeatureUnion(n_jobs=None,\n",
       "                              transformer_list=[('tfidf',\n",
       "                                                 TfidfVectorizer(analyzer='word',\n",
       "                                                                 binary=False,\n",
       "                                                                 decode_error='strict',\n",
       "                                                                 dtype=<class 'numpy.float64'>,\n",
       "                                                                 encoding='utf-8',\n",
       "                                                                 input='content',\n",
       "                                                                 lowercase=True,\n",
       "                                                                 max_df=0.9,\n",
       "                                                                 max_features=100,\n",
       "                                                                 min_df=1,\n",
       "                                                                 ngram_range=(1,\n",
       "                                                                              2),\n",
       "                                                                 norm='l2',\n",
       "                                                                 preprocessor=None,\n",
       "                                                                 smooth_idf=True,\n",
       "                                                                 stop_words=None,\n",
       "                                                                 st...\n",
       "                                                                 encoding='utf-8',\n",
       "                                                                 input='content',\n",
       "                                                                 lowercase=True,\n",
       "                                                                 max_df=0.6,\n",
       "                                                                 max_features=5000,\n",
       "                                                                 min_df=1,\n",
       "                                                                 ngram_range=(1,\n",
       "                                                                              1),\n",
       "                                                                 preprocessor=None,\n",
       "                                                                 stop_words='english',\n",
       "                                                                 strip_accents=None,\n",
       "                                                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                                                 tokenizer=None,\n",
       "                                                                 vocabulary=None))],\n",
       "                              transformer_weights=None, verbose=False)),\n",
       "                ('nb',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save best model as gs_model.\n",
    "\n",
    "gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9624436244362443"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate model on training data.\n",
    "gs.score(X_train['selftext'], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9545007378258731"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate model on testing data.\n",
    "gs.score(X_test['selftext'], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.550664\n",
       "0    0.449336\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = gs.predict(X_test['selftext'])\n",
    "pred = pd.DataFrame(pred)\n",
    "pred[0].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.557304\n",
       "0    0.442696\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Baseline score\n",
    "\n",
    "y_test.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
